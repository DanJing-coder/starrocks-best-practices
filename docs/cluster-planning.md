# 集群规划

在部署 StarRocks 之前，进行科学合理的集群规划是确保系统上线后能够稳定、高效运行的基石。一份周全的规划可以避免后期因资源不足或配置不当导致的性能瓶颈和运维难题，并为未来的扩展性提供保障。

本章节将从部署模式选择、组件角色、硬件选型、容量评估和组件部署策略等核心方面，为您提供一份系统性的规划指南。

## 1. 部署模式选择

StarRocks 支持两种主流的部署模式，这是规划时需要做出的第一个、也是最重要的决策。

*   **存算一体 (Shared-Nothing):** 这是最经典和常用的部署模式。数据存储和计算都由 BE 节点完成。
    *   **优点:** 架构简单，运维成本低，数据本地化计算带来极致的查询性能。
    *   **缺点:** 扩容时计算和存储资源必须同时扩容，弹性较差。
    *   **适用场景:** 私有化部署、对查询性能有极致要求的核心业务、业务负载相对稳定。

*   **存算分离 (Shared-Data):** 数据存储在远端可靠的对象存储（如 AWS S3, Google GCS, Azure Blob Storage, 或 MinIO）上，计算由独立的 CN 节点完成。
    *   **优点:** 计算和存储资源可以独立按需扩缩容，弹性极佳，能有效应对业务高峰低谷，降低存储成本。
    *   **缺点:** 相比存算一体，由于网络开销，查询性能可能会有一定损耗（但 StarRocks 的热数据缓存机制可以很大程度上缓解此问题）。
    *   **适用场景:** 公有云环境、业务负载波动大（有明显潮汐现象）、对弹性要求高的场景、或希望利用廉价对象存储的场景。对于多业务混合且需要严格资源隔离的场景，存算分离模式配合资源组也更具优势。

> **最佳实践:** 对于大多数私有化部署场景，建议优先选择**存算一体**模式以获得最佳性能和简化的运维。对于云上部署或需要极致弹性的场景，**存算分离**是更优的选择。本章后续内容主要围绕更通用的存算一体模式展开。

## 2. 组件角色

了解核心组件的职责有助于我们进行更合理的资源配置。

*   **Frontend (FE):** FE 节点负责元数据管理、客户端连接管理、查询解析与规划、任务调度等工作。它本身不存储数据，是集群的“大脑”。FE 节点的内存和 CPU 资源主要消耗在元数据管理和查询规划上。
*   **Backend (BE):** 在存算一体模式下，BE 节点负责数据的存储与管理、查询的具体执行、数据导入与导出等工作。它是集群的“主力”，承担了所有的数据处理和 I/O 负载。BE 节点的性能直接决定了整个集群的性能。
*   **Compute Node (CN):** 在存算分离模式下，CN 节点是可选的无状态计算节点，负责执行 SQL 查询。它本身不存储数据，可以根据查询负载进行快速弹性伸缩，从而实现极致的弹性。

## 3. 硬件选型

硬件选型是集群规划中至关重要的一环。遵循以下建议可以为 StarRocks 提供一个坚实的物理基础。

### CPU

StarRocks 的查询执行高度依赖 CPU 的向量化计算能力。

*   **指令集要求:** **必须**选用支持 **AVX2 指令集** 的 x86-64 架构 CPU。这是保证 StarRocks 高性能运行的硬性要求。
*   **架构选型与性能参考:**
    *   **Intel / AMD (x86_64):** 主流选择，如 Intel Xeon Gold 系列或 AMD EPYC 系列，性能基准为 100%。
    *   **海光 (x86_64):** 国产信创首选，如 7000 系列，支持 AVX2，性能约为 Intel 同级别 CPU 的 70%。
    *   **华为鲲鹏 (ARM):** 性能约为 Intel 同级别 CPU 的 60%，且**不支持 AVX2**，需要使用单独编译的 BE 版。
    *   **AWS Graviton (ARM):** 如 Graviton3，性能优异，可达 Intel 同级别 CPU 的 100%-114% (StarRocks 3.1+ 版本有专项优化)，需要使用 ARM 版本的安装包。
*   **FE 节点:** 对 CPU 要求相对较低，通常 **8-16 核** 即可满足需求。
*   **BE/CN 节点:** 是计算密集型节点，对 CPU 要求高。推荐 **32-64 核** 的高主频 CPU。核心数越多，并发查询处理能力越强；主频越高，单个查询的计算速度越快。不建议低于 16 核。

### 内存

内存主要用于数据缓存、查询计算以及元数据存储。

*   **FE 节点:** 内存消耗主要与元数据规模（库、表、分区、Tablet 的数量）成正比。一个 Tablet 元信息约占 1KB 内存。假设一个集群有 100 万个 Tablet，则元数据约消耗 1GB 内存。
    *   **通用配置:** 建议 **32 GB** 起步。对于超大规模集群（例如 300 万 Tablet 约需 110GB 内存），建议配置 **128 GB** 或更高，并配合 G1 GC 策略。
*   **BE 节点:** 内存是影响查询性能的关键因素。越大的内存可以缓存越多的热点数据，减少磁盘 I/O，从而大幅提升查询速度。
    *   **最低配置:** 64 GB。
    *   **推荐配置:** 128 GB 或更高。
    *   **内存与 CPU 配比:** 推荐内存与 CPU 核数的比例为 **4:1** (例如 32 核 CPU 配 128GB 内存)，最低不应低于 2:1。

### 磁盘

磁盘是数据持久化存储的地方，其 I/O 性能直接影响数据导入和查询性能。

*   **FE 节点:** FE 的元数据存储（BDB JE）对磁盘 I/O 有一定要求，但数据量不大。推荐使用 **SSD** 以获得更快的元数据响应速度，容量 200GB 以上即可。
*   **BE 节点:** BE 节点是 I/O 密集型节点，强烈推荐使用高性能 SSD。
    *   **类型:** 推荐使用 **NVMe SSD**，其次是普通企业级 SSD。**实时场景严禁在生产环境中使用 HDD (SATA/SAS)**，其缓慢的随机 I/O 会导致查询和写入性能严重下降和系统不稳定。
    *   **容量与数量:** 单块磁盘容量不建议超过 4TB，单 BE 节点挂载 4-6 块盘为宜，以利用多路径 I/O 提升吞吐。
    *   **RAID:** **不建议** 使用 RAID。StarRocks 在应用层通过多副本机制保证数据可靠性。使用 JBOD (Just a Bunch of Disks) 模式即可，由 StarRocks 来管理多块磁盘。
    *   **性能要求:** 对于有实时更新（尤其是主键模型）的场景，建议使用 `fio` 等工具测试磁盘性能，确保在混合读写场景下，单盘 IOPS > 2000，吞吐 > 200 MB/s。

### 网络

网络负责节点间的数据传输，包括查询执行过程中的数据 shuffle、副本同步等。

*   **通用建议:** 推荐使用 **万兆（10 Gbps）或更高** 的网络环境，并确保交换机无阻塞。对于大规模集群或高并发查询场景，25 Gbps 或更高的网络可以有效避免网络成为瓶颈。

### 部署环境考量

*   **物理机 vs. 虚拟机/容器:**
    *   **物理机:** 性能最佳，无虚拟化损耗。
    *   **虚拟机 (KVM):** 存在 10%-20% 的性能损耗。
    *   **容器 (Kubernetes):** 存在 10%-15% 的性能损耗，主要在网络和存储 I/O。
    *   **嵌套部署:** 如果在虚拟机中再运行 K8s，性能损耗会叠加，需谨慎评估。

## 4. 容量评估

容量评估是规划的核心，旨在科学地估算出集群所需的节点数量和总存储空间。

### 4.1 评估输入：收集业务与技术指标

在计算前，请先向业务方收集以下关键信息：

*   **业务场景:** BI 报表、Ad-hoc 自助分析、用户画像、CDC 实时同步等。
*   **数据体量:**
    *   总历史数据量 (原始格式，如 CSV/Parquet)。
    *   每日新增数据量 (行数或大小)。
    *   数据保留周期 (例如 180 天、365 天)。
*   **查询负载:**
    *   **查询并发 (QPS):** 高峰期的并发查询数。建议评估连接池模式下的并发数（通常 50 - 100）。
    *   **查询复杂度:** 评估典型查询扫描的数据行数，可分为三类：简单查询 ( &lt; 1000 万行)、复杂查询 (1000 - 5000 万行)、重度分析 ( &gt; 5000 万行)。
    *   **响应时间要求 (SLA):** 例如 P99 响应时间 &lt; 1s。
    *   **负载模式:** 是否有明显的潮汐现象 (如月报、年报导致的高峰)。

### 4.2 步骤一：存储容量评估

**1. 评估参数:**

*   **数据压缩比:** StarRocks 采用列式存储和多种压缩算法，压缩比通常在 **3:1 到 5:1** 之间 (例如 100GB CSV 导入后约 20-33GB)。保守起见，我们按 **3:1** 估算。
*   **副本数:** 生产环境标准为 **3** 副本。
*   **空间开销系数:** 用于预留索引、元数据、Compaction 和版本迭代的临时空间，通常建议取 **30%** (即 0.3)。
*   **磁盘空间使用率阈值:** 为保证系统稳定和后台任务顺利进行，磁盘使用率不应过高，建议保持在 **70%** 以下。

**2. 计算步骤:**

1.  **计算总原始数据量:** `总原始数据量 = 总历史数据量 + (每日新增数据量 × 数据保留周期)`
2.  **计算压缩后的单副本数据大小:** `单副本数据大小 = 总原始数据量 / 数据压缩比`
3.  **计算多副本及开销后的总物理空间:** `总物理空间 = 单副本数据大小 × 副本数 × (1 + 空间开销系数)`
4.  **计算集群所需的总磁盘容量:** `集群总磁盘容量 = 总物理空间 / 磁盘空间使用率阈值`
5.  **计算所需 BE 节点数:** `BE 节点数 = CEILING(集群总磁盘容量 / 单节点磁盘容量)` (CEILING 表示向上取整)

**3. 示例：**

*   **业务输入:**
    *   历史数据: 5 TB
    *   每日新增: 50 GB
    *   保留周期: 365 天
*   **计算过程:**
    1.  总原始数据量 = `5 TB + (50 GB * 365)` ≈ `5 TB + 17.8 TB` = **22.8 TB**
    2.  单副本大小 = `22.8 TB / 3` = **7.6 TB**
    3.  总物理空间 = `7.6 TB * 3 * (1 + 0.3)` = **29.64 TB**
    4.  集群总磁盘容量 = `29.64 TB / 0.7` ≈ **42.34 TB**
    5.  假设选用单机 8TB 磁盘的 BE 机型，则 BE 节点数 = `CEILING(42.34 / 8)` = **6 个**

**结论:** 根据存储评估，需要部署 **6 个** BE 节点（每节点配置 8TB SSD）来满足一年的存储需求。

### 4.3 步骤二：计算资源评估

计算资源的评估比存储更复杂，因为它与查询模式紧密相关。

*   **核心关系:** StarRocks 的查询能力与集群总 CPU 核数正相关。增加 BE 节点数量可以线性提升集群的整体服务能力。
*   **QPS 评估的复杂性:** OLAP 场景的 SQL 复杂度差异巨大，无法像 OLTP 那样用简单的公式精确计算 QPS。一个扫描百亿数据的复杂 SQL 和一个点查 SQL 消耗的资源可能有成千上万倍的差距。
*   **实际 QPS vs. 理论 QPS:** 即使对于固定耗时的查询，实际 QPS 也通常低于 `并发数 / 平均耗时` 的理论值，因为存在调度等待、资源排队等系统开销。

**评估方法：**

1.  **遵循“先满足存储，再优化计算”原则:** 先根据步骤一计算出满足存储需求的 BE 节点数。
2.  **参考模板匹配场景:** 结合下面的《集群体量规格参考模版》，找到与您业务最匹配的场景，以此为基线配置。
3.  **上线后验证与调优:** 在系统上线后，通过监控（如 CPU 使用率、查询延迟、`fe/log/fe.audit.log`）来判断计算资源是否成为瓶颈。如果 CPU 长期处于高位，或查询延迟不达标，应考虑增加 BE 节点。

### 4.4 步骤三：集群体量规格参考模版

下表提供了不同业务场景下的典型配置，可作为您规划的起点。

| 适用场景 | 固定BI报表 | 实时更新+复杂Adhoc查询 | 金融级实时分析 | 实时海量数据摄入 (IoT) |
| :--- | :--- | :--- | :--- | :--- |
| **主要负荷** | 计算密集型 | 计算+IO密集型 | 计算+IO密集型 | 重度IO密集型 |
| **BE 节点数** | 3 ~ 10 | 11 ~ 50 | 51 ~ 100 | 100+ |
| **单副本热数据** | &lt; 10TB | 10 ~ 50TB | 50 ~ 100TB | &lt; 5PB |
| **日增量 (行)** | 千万级 | 千万~百亿级 | 千亿级 | 万亿级 |
| **FE 节点数** | 3 | 3 | 5 | 5+ |
| **FE 内存** | 64GB+ | 64~128GB | 128GB+ | 256GB+ |
| **BE CPU (核)** | 32+ | 32~96 | 32~96 | 64~96 |
| **BE 内存** | 128GB+ | 128~256GB | 256~512GB | 256~512GB |
| **BE 磁盘类型** | NVMe SSD | NVMe SSD | NVMe SSD | NVMe SSD |
| **峰值 QPS** | 10~200 | 200~2k | 1~2k | 2~10k |
| **P99 简单查询** | &lt; 1s | &lt; 1s | &lt; 1s | &lt; 1s |
| **P99 复杂查询** | &lt; 10s | &lt; 10s | &lt; 10s | &lt; 10s |

## 5. 组件部署策略

### FE 节点部署

*   **高可用:** 生产环境**必须**部署 **3 个或以上（奇数个）** FE 节点，以组成一个高可用的集群。1 个 Leader 负责写操作，其他 Follower 负责读操作并作为备份，通过 BDB JE 协议保证元数据一致性。
*   **FE Observer:** 对于读多写少的场景，可以额外部署 FE Observer 节点来扩展查询规划能力和连接数，而不影响 Leader 的选举和写入性能。
*   **物理隔离:** 3 个 FE 节点应部署在不同的物理机或虚拟机上，避免单点物理故障。
*   **资源隔离:** **不建议** 将 FE 与 BE 部署在同一台服务器上，以避免资源争抢。

### BE 节点部署

*   **高可用与数量:** 生产环境**至少**需要 **3 个** BE 节点。**推荐部署 4 个或以上**，这样在有一个节点宕机时，集群仍能正常执行 DDL 操作，且副本恢复的压力更小。
*   **物理/机架隔离:** BE 节点应部署在不同的物理机上。如果条件允许，建议进行机架感知（Rack-aware）部署，将不同副本分布在不同机架上，以抵御机架级别的故障。
*   **对等部署:** 所有 BE 节点的硬件配置应保持一致（CPU、内存、磁盘），以避免因“木桶效应”导致部分节点成为性能瓶颈。**禁止**在同一个集群内混合部署 ARM 和 x86 架构的 BE 节点。

## 6. 规划清单总结

在完成规划后，请对照以下清单进行检查，确保所有关键点都已考虑周全：

- [ ] **部署模式已确定：** 存算一体 vs. 存算分离？
- [ ] **业务指标已收集：** 数据量、增长率、QPS 等。
- [ ] **硬件配置已选定：** CPU、内存、磁盘、网络、部署环境（物理机/VM/K8s）是否满足最佳实践要求？
- [ ] **存储容量已评估：** BE 节点数量和磁盘容量是否考虑了压缩、副本、开销和预留空间？
- [ ] **计算资源已评估：** 当前硬件配置是否能初步满足查询需求？
- [ ] **高可用方案已设计：** FE 和 BE 节点数量和部署方式是否满足高可用要求？
- [ ] **未来扩展性已考虑：** 当前规划是否为未来 1-2 年的业务增长留有余地？

---

周全的集群规划是 StarRocks 项目成功的第一步。希望本章的指南能帮助您构建一个坚实、高效、可扩展的集群基础。
